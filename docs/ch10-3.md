## Lagrange Multipliers

When we have a problem of variation where one function is maximized or minimized subject to a constraint imposed by another function

```math
f(x_1, \ldots x_n) \rightarrow \text{ function }
```
```math
g(x_1, \ldots, x_n) = 0 \rightarrow \text { constraint }
```

Without the constraint we would have the problem 
```math
\delta f = \pdv{f}{x_1} \delta x_1 + \ldots \pdv{f}{x_n} \delta x_n
```
```math
\pdv{f}{x_i} = 0 \qquad i = 1 \ldots n
```
and apply any of our multivariate optimization strategies to solve. This can be hard. Luckily, at any stationary point of the function that also satisfies the constraint, the gradient of the function at that point can be expressed as a linear combination of the gradients of the constraints at that point.

```math
\pdv{f}{x_i} - \lambda \pdv{g}{x_i} = 0
```

With a constraint $` g = 0 `$, the change in $` f `$ becomes a change in the functional of $` f `$ and $` g `$.
```math
f, g \rightarrow h(f(x_1, \ldots, x_{n-1}), g(x_1, \ldots, x_{n-1})) = h(x_1, \ldots, x_{n-1}) = 0
```
```math
\pdv{h}{x_i} \rightarrow i = 1 \rightarrow n-1
```
At the stationary points we're looking for, 

Simplest example: Find the maximum area of a rectangle with perimeter $` 4a `$

```math
f = A = x_1 x_2
```
```math
g = 0 = 2x_1 + 2x_2 - 4a
```
```math
\delta f = x_2 \delta x_1 + x_1 \delta x_2
```
```math
\lambda \delta g = \lambda 2 \delta x_1 + \lambda 2 \delta x_2
```
```math
x_2 + 2\lambda = 0 \qquad x_1 + 2\lambda = 0
```
```math
x_2 = - 2 \lambda \qquad x_1 = - 2 \lambda
```
```math
\rightarrow - 4 \lambda - 4 \lambda - 4 a = 0 \rightarrow \lambda = - \frac{a}{2}
```
