# **10-1** Statistical Mechanics

<!-- TA office: AERB 430 -->

Before getting started with real plasma physics concepts, we need to quickly review some statistical mechanics with the goal of deriving the Maxwell-Boltzmann distribution.



As we all know, if we have \( N \) unique objects, there are \( N! \) ways of arranging them. If \( n_1 \ldots n_k \) are identical with N things, the number of combinations is 
$$
\frac{N!}{n_1 ! \ldots n_k !}
$$

What does probability have to do with a velocity distribution? Consider the one-dimensional random walk, in which we take \( N \) steps, and at each step we move in a random direction. We take \( n_r \) steps to the right and \( N - n_r \) steps to the left. For a random walk we assume the probability of going in each direction is the same
$$
P_r = p = \frac{1}{2} \qquad P_l = q =  \frac{1}{2}
$$
After taking \( N \)  steps, the probability of taking \( n_r \) steps to the right and \( (N - n_r) \) steps to the left is
$$
P(n_r) = \frac{N !}{n_r ! (N - n_r)!} \cdot p^{n_r} q^{n_r}
$$

We are also interested in the final destination, which is the net number of steps to the right
$$
m_r = n_r - n_l = 2 n_r - N
$$

If we have a bias to move in a particular direction, then \( p \neq q \) and the distribution \( P(n_r) \) will be shifted towards the bias.

Of course, if we have a very large \( N \) we aren't going to be able to compute \( P(n_r) \) directly. We've got our handy dandy natural logarithm to help us.

$$
\ln N! = \ln N + \ln (N - 1) + \ldots  + 1
$$

$$
\ln (N + 1) ! = \ln (N + 1) + \ln N!
$$

$$
\pdv{\ln N!}{N} \approx \frac{\ln(N+1)! - \ln N!}{N - (N-1)} = \ln(N+1) \approx \ln N
$$

Recall our expression for the probability distribution

$$
P(n_r) = \frac{N!}{n_r !(N - n_r)!} p^{n_r} q^{N- n_r}
$$
$$
\rightarrow \quad \ln P(n_r) = \ln N! - \ln n_r ! - \ln ((N-n_r)!) + n_r \ln p + (N - n_r) \ln q
$$

Now we apply the little log trick \( \pdv{\ln N!}{N} \approx \ln N \) 
$$
\dv{\ln P(n_r)}{n_r} = - \ln r + \ln (N - n_r) + \ln p - \ln q = 0
$$
$$
\qquad \rightarrow \qquad \ln \left( \frac{N - n_r}{n_r} \frac{p}{q} \right) = 0
$$
$$
(N - n_r)p = n_r q
$$
$$
Np = n_r(p + q) 
$$
$$
\overline{n_r} = Np
$$
Surprise surprise, the probability of getting a certain result is just the expectation value of that result. Can we also say anything about the width of the distribution? 
Taylor expand about \( \overline{n_r} = N p \) and define \( \eta \) by \( n_r = N p + \eta \) 
$$
\ln (P(n_r)) = \ln(P(Np)) + B_1 \eta + \frac{1}{2} B_2 \eta^2 + \frac{1}{6} B_3 \eta^4
$$
$$
B_k = \frac{d^k \ln P(n_r)}{d n_r ^k} 
$$
$$
\dv{\ln P(n_r)}{n_r} = - \ln n_r + \ln (N - n_r) + \ln p - \ln q
$$
$$
B_1 = - \ln(n_r) + \ln (N - n_r) + \ln p - \ln q
$$
$$
= - \ln (Np) + \ln(N - Np) = 0
$$
$$
B_2 = - \frac{1}{n_r} - \frac{1}{N - n_r} = - \frac{1}{Np} - \frac{1}{N - Np} = -\frac{1}{Nqp}
$$
$$
B_3 = \frac{1}{n_r ^2} - \frac{1}{(N - n_r)^2} \approx \frac{1}{N^2}
$$
$$
B_4 \approx \frac{1}{N_3}
$$

The expansion converges for \( N >> \eta \) 
$$
\ln(P(n_r)) \approx \ln (P(Np)) - \frac{1}{2} \frac{1}{Nqp} \eta ^2
$$
or
$$
P(n_r) = P(Np) e^{- \frac{\eta ^2}{2 Npq}} \qquad P_0 = P(Np)
$$
$$
P(\eta) = P_0 e^{-\frac{\eta ^2}{2Npq}}
$$

So what's the width?

$$
\langle \eta ^2 \rangle = \frac{ \int_{-\infty} ^\infty \eta ^2 P(\eta) \dd \eta }{\int_{-infty}^\infty P(\eta) \dd \eta} = \frac{ \frac{ \sqrt{\pi}}{4 \left( \frac{1}{2 N pq} \right) ^{3/2}}}{\frac{ \sqrt{\pi}}{2 \left( \frac{1}{2 N pq} \right)^{1/2}}} = \frac{2 Npq}{2} = Npq
$$

So the uncertainty is \( \delta n_r = \sqrt{ N pq} \approx \frac{1}{2} \sqrt{N} \). For a very large N, the relative uncertainty \( \delta n_r / N \approx \frac{1}{2 \sqrt{N}} \) diminishes and the distribution (centered at \( Np \) ) gets very narrow. The signal-to-noise will go as \( \frac{1}{\sqrt{N}} \).


## **10.1.2** Lagrange Multipliers

When we have a problem of variation where one function is maximized or minimized subject to a constraint imposed by another function

$$
f(x_1, \ldots x_n) \rightarrow \text{ function }
$$
$$
g(x_1, \ldots, x_n) = 0 \rightarrow \text { constraint }
$$

Without the constraint we would have the problem 
$$
\delta f = \pdv{f}{x_1} \delta x_1 + \ldots \pdv{f}{x_n} \delta x_n
$$
$$
\pdv{f}{x_i} = 0 \qquad i = 1 \ldots n
$$
and apply any of our multivariate optimization strategies to solve. This can be hard. Luckily, at any stationary point of the function that also satisfies the constraint, the gradient of the function at that point can be expressed as a linear combination of the gradients of the constraints at that point.

$$
\pdv{f}{x_i} - \lambda \pdv{g}{x_i} = 0
$$

With a constraint \( g = 0 \), the change in \( f \) becomes a change in the functional of \( f \) and \( g \).
$$
f, g \rightarrow h(f(x_1, \ldots, x_{n-1}), g(x_1, \ldots, x_{n-1})) = h(x_1, \ldots, x_{n-1}) = 0
$$
$$
\pdv{h}{x_i} \rightarrow i = 1 \rightarrow n-1
$$
At the stationary points we're looking for, 

Simplest example: Find the maximum area of a rectangle with perimeter \( 4a \)

$$
f = A = x_1 x_2
$$
$$
g = 0 = 2x_1 + 2x_2 - 4a
$$
$$
\delta f = x_2 \delta x_1 + x_1 \delta x_2
$$
$$
\lambda \delta g = \lambda 2 \delta x_1 + \lambda 2 \delta x_2
$$
$$
x_2 + 2\lambda = 0 \qquad x_1 + 2\lambda = 0
$$
$$
x_2 = - 2 \lambda \qquad x_1 = - 2 \lambda
$$
$$
\rightarrow - 4 \lambda - 4 \lambda - 4 a = 0 \rightarrow \lambda = - \frac{a}{2}
$$
