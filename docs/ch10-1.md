# **10-1** 2020-01-06

<!-- TA office: AERB 430 -->

Before getting started with real plasma physics concepts, we need to quickly review some statistical mechanics with the goal of deriving the Maxwell-Boltzmann distribution.



As we all know, if we have \( N \) unique objects, there are \( N! \) ways of arranging them. If \( n_1 \ldots n_k \) are identical with N things, the number of combinations is 
$$
\frac{N!}{n_1 ! \ldots n_k !}
$$

What does probability have to do with a velocity distribution? Consider the one-dimensional random walk, in which we take \( N \) steps, and at each step we move in a random direction. We take \( n_r \) steps to the right and \( N - n_r \) steps to the left. For a random walk we assume the probability of going in each direction is the same
$$
P_r = p = \frac{1}{2} \qquad P_l = q =  \frac{1}{2}
$$
After taking \( N \)  steps, the probability of taking \( n_r \) steps to the right and \( (N - n_r) \) steps to the left is
$$
P(n_r) = \frac{N !}{n_r ! (N - n_r)!} \cdot p^{n_r} q^{n_r}
$$

We are also interested in the final destination, which is the net number of steps to the right
$$
m_r = n_r - n_l = 2 n_r - N
$$

If we have a bias to move in a particular direction, then \( p \neq q \) and the distribution \( P(n_r) \) will be shifted towards the bias.

Of course, if we have a very large \( N \) we aren't going to be able to compute \( P(n_r) \) directly. We've got our handy dandy natural logarithm to help us.

$$
\ln N! = \ln N + \ln (N - 1) + \ldots  + 1
$$

$$
\ln (N + 1) ! = \ln (N + 1) + \ln N!
$$

$$
\pdv{\ln N!}{N} \approx \frac{\ln(N+1)! - \ln N!}{N - (N-1)} = \ln(N+1) \approx \ln N
$$

Recall our expression for the probability distribution

$$
P(n_r) = \frac{N!}{n_r !(N - n_r)!} p^{n_r} q^{N- n_r}
$$
$$
\rightarrow \quad \ln P(n_r) = \ln N! - \ln n_r ! - \ln ((N-n_r)!) + n_r \ln p + (N - n_r) \ln q
$$

Now we apply the little log trick \( \pdv{\ln N!}{N} \approx \ln N \) 
$$
\dv{\ln P(n_r)}{n_r} = - \ln r + \ln (N - n_r) + \ln p - \ln q = 0
$$
$$
\qquad \rightarrow \qquad \ln \left( \frac{N - n_r}{n_r} \frac{p}{q} \right) = 0
$$
$$
(N - n_r)p = n_r q
$$
$$
Np = n_r(p + q) 
$$
$$
\overline{n_r} = Np
$$
Surprise surprise, the probability of getting a certain result is just the expectation value of that result. Can we also say anything about the width of the distribution? 
Taylor expand about \( \overline{n_r} = N p \) and define \( \eta \) by \( n_r = N p + \eta \) 
$$
\ln (P(n_r)) = \ln(P(Np)) + B_1 \eta + \frac{1}{2} B_2 \eta^2 + \frac{1}{6} B_3 \eta^4
$$
$$
B_k = \frac{d^k \ln P(n_r)}{d n_r ^k} 
$$
$$
\dv{\ln P(n_r)}{n_r} = - \ln n_r + \ln (N - n_r) + \ln p - \ln q
$$
$$
B_1 = - \ln(n_r) + \ln (N - n_r) + \ln p - \ln q
$$
$$
= - \ln (Np) + \ln(N - Np) = 0
$$
$$
B_2 = - \frac{1}{n_r} - \frac{1}{N - n_r} = - \frac{1}{Np} - \frac{1}{N - Np} = -\frac{1}{Nqp}
$$
$$
B_3 = \frac{1}{n_r ^2} - \frac{1}{(N - n_r)^2} \approx \frac{1}{N^2}
$$
$$
B_4 \approx \frac{1}{N_3}
$$

The expansion converges for \( N >> \eta \) 
$$
\ln(P(n_r)) \approx \ln (P(Np)) - \frac{1}{2} \frac{1}{Nqp} \eta ^2
$$
or
$$
P(n_r) = P(Np) e^{- \frac{\eta ^2}{2 Npq}} \qquad P_0 = P(Np)
$$
$$
P(\eta) = P_0 e^{-\frac{\eta ^2}{2Npq}}
$$